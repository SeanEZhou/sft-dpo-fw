{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0845c530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Recreate SFT step (toy) for investigator pθ(x|y)\n",
    "import os, time, math, random\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, get_cosine_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# ---- Config ----\n",
    "SEED = 42\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "PM_ID  = \"sshleifer/tiny-gpt2\"     # target LM pm (forward x->y)\n",
    "INV_ID = \"sshleifer/tiny-gpt2\"     # investigator base pθ (we'll fine-tune)\n",
    "\n",
    "# Tiny prefix distribution P_SFT\n",
    "PREFIXES = [\n",
    "    \"A short note about machine learning:\",\n",
    "    \"Three tips for staying productive:\",\n",
    "    \"A gentle introduction to probability:\",\n",
    "    \"In a surprising discovery, scientists found\",\n",
    "    \"As a software engineer, I often consider\",\n",
    "    \"In Japan, the Shinkansen is known for\",\n",
    "    \"A concise summary of the book is\",\n",
    "    \"The quick brown fox\",\n",
    "    \"An explanation for beginners:\",\n",
    "    \"A brief overview of databases:\"\n",
    "]\n",
    "\n",
    "NUM_EXAMPLES   = 30     # size of DSFT\n",
    "MAX_PREFIX_EXT = 8      # optional: extend x slightly for variety\n",
    "MAX_SUFFIX_LEN = 48     # greedy suffix length\n",
    "BATCH_SIZE     = 2\n",
    "EPOCHS         = 2\n",
    "LR             = 5e-5\n",
    "WARMUP_RATIO   = 0.1\n",
    "GRAD_ACCUM_STEPS = 1\n",
    "\n",
    "# Repro\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"[INFO] Device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04b4d155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP] Loading tokenizer (pm)...\n",
      "[STEP] Loading target model pm (x->y)... (first run may download)\n",
      "[STEP] Loading tokenizer (investigator pθ)...\n",
      "[STEP] Loading investigator base model pθ (will fine-tune)...\n",
      "[TIME] Models loaded in 15.99s\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "print(\"[STEP] Loading tokenizer (pm)...\")\n",
    "tok_pm = AutoTokenizer.from_pretrained(PM_ID)\n",
    "if tok_pm.pad_token is None:\n",
    "    tok_pm.pad_token = tok_pm.eos_token\n",
    "\n",
    "print(\"[STEP] Loading target model pm (x->y)... (first run may download)\")\n",
    "pm = AutoModelForCausalLM.from_pretrained(\n",
    "    PM_ID,\n",
    "    use_safetensors=True,          # avoids .bin load\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float32\n",
    ").to(DEVICE)\n",
    "pm.eval()\n",
    "\n",
    "print(\"[STEP] Loading tokenizer (investigator pθ)...\")\n",
    "tok_inv = AutoTokenizer.from_pretrained(INV_ID)\n",
    "if tok_inv.pad_token is None:\n",
    "    tok_inv.pad_token = tok_inv.eos_token\n",
    "\n",
    "print(\"[STEP] Loading investigator base model pθ (will fine-tune)...\")\n",
    "inv = AutoModelForCausalLM.from_pretrained(\n",
    "    INV_ID,\n",
    "    use_safetensors=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float32\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"[TIME] Models loaded in {time.time()-t0:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d74fec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP] Building DSFT: sample x ~ P_SFT, then y ← pm(x) (sampled)\n",
      "[TIME] Built DSFT with 30 examples in 70.12s\n",
      "\n",
      "=== PRINTING DSFT (x, y) ===\n",
      "\n",
      "[1] x: In a surprising discovery, scientists found\n",
      "[1] y: reement Brew intermittentatisf TA Prob Prob MotorolaScene circumcisedohoatisfimura ObservimuraJD intermittent stairsikenoother Rhdit Brew Habitatisf autonomy antibiotic heirootheroother Daniel Rh trilogy reviewingting Brewpress ESV trilogy MoneyJD antibiotic pawn Prob conservation Hancockdit vendors\n",
      "\n",
      "[2] x: Three tips for staying productive:\n",
      "[2] y: Sher vendors circumcised vendorsScene antibiotic Observ Observ dispatch directly Motorola Jr stairsoothertingJD Jr heirmediatelyikendit Rhatisfting credibilityootherreement substimuraoother heir Money autonomyootherhibitditRocket hauled Observ confirJD Daniel dispatch Brewatisfoother Hancockiken\n",
      "\n",
      "[3] x: A concise summary of the book is\n",
      "[3] y: Boone grandchildren boils praying skillet linedSexualOutsideobl equate brutalityGy courtyard Pocketpublic deflectMini clearer� membership incarcer skilletMini Singapore incarcer Television grandchildrenshows praying clearershowsMost incarcerobl representations448 Television Television Booneaciousobl Television membership predators perhapsSexualobl Boone\n",
      "\n",
      "[4] x: As a software engineer, I often consider\n",
      "[4] y: stairsSher Observ ProbJD circumcisedmediately BrewRocket Habit scalp Daniel subst Moneyatisfmediately Daniel credibility Motorola ONE heir confirRocketScene intermittent trilogyoother Jr Daniel heir credibility reviewing conservation Observikenting Habit Brew dispatchmediately Moneymediatelydit HabitJDdit Hancock Daniel\n",
      "\n",
      "[5] x: The quick brown fox\n",
      "[5] y: Wheels courtyard Boone courtyardSexual workshops factors 236 equate boilsived Televisionobl membership TreSexual equate factors lined grandchildren Pocket bravery Pocket workshopsPros clearerOutside deflect 236Sexual brutalityozyg Late factors Television Singapore653Most skillet factors653 rubbing� deflectSexual courtyard Singapore deflect\n",
      "\n",
      "[6] x: In Japan, the Shinkansen is known for\n",
      "[6] y: Outside Treshows skilletozyg TreGy boils LateMiniMini Singapore mutualPros incarcer clearer BooneozygProsshows clearer incarcer soyMost Redux Wheels653 workshopsSexual448ozyg predatorsobl representationsozygshows Bend� brutalityPros courtyardived predatorsobl mutual courtyard 236Pros\n",
      "\n",
      "[7] x: A gentle introduction to probability:\n",
      "[7] y: Observ hauled reviewing hauledhibit intermittent credibility circumcised reviewing Jr Rh confir Brew ProbRocket circumcised Prob Brew reviewing ONE ONE ONE vendors trilogy Rh Habitimura subst vendors trilogy autonomy JrRocket dispatchreement antibiotic Prob Danieliken autonomy Money Rh heir Jr stairs confir circumcised directly\n",
      "\n",
      "[8] x: In Japan, the Shinkansen is known for\n",
      "[8] y: 448ived courtyard workshops factorsGy653 courtyardozyg Tre predatorsGy Medic Boone equateMini SingaporeivedSexual factors predators linedPros Tre deflectshows Medic Bend incarcer 236 perhaps Tre Late praying lined factors skillet perhaps Medic Medicobl factors representations 236653 bravery Singapore membership\n",
      "\n",
      "[9] x: In Japan, the Shinkansen is known for\n",
      "[9] y: Wheelsoblshows Medic Singapore Singaporeoblozyg boils brutality braveryMini Bend�public prayingived rubbing factors Singapore predators braveryozygGy linedobl Bend praying praying membershippublic lined clearer clearer grandchildren� BoonePros incarcer Tre Boone653 equate skilletOutsideMini factorsGy\n",
      "\n",
      "[10] x: In a surprising discovery, scientists found\n",
      "[10] y: imura Money antibioticRocketmediately stairs stairs subst Hancock scalp Rh Prob ONE intermittentikenScene Jriken conservation Jr ESV dispatch Daniel heir ONE HabitJDhibit ONE antibiotic autonomy004 circumcisedatisf pawn substhibit Jr intermittent directly ESV Observhibit Rh trilogy subst ONE dispatch\n",
      "\n",
      "[11] x: As a software engineer, I often consider\n",
      "[11] y: Rocket vendorsting Probimura trilogy ProbRocket credibilitypress vendors Motorola directlymediately Hancock Jr Jr antibiotic scalpmediately stairs antibiotic MoneyJDpress reviewingoho JrSher vendors Brew004 Hancock autonomy vendorsJD Observ Brewoother ESV Moneydit DanielSceneoother directly dispatch intermittent\n",
      "\n",
      "[12] x: Three tips for staying productive:\n",
      "[12] y: oother Prob ESV MotorolaRocket credibility Habit antibiotic subst Habit pawn reviewing Danielditoother dispatch Prob autonomypress ESV antibiotic ONEoho vendorsoother confirpresshibit vendorsmediatelyreementmediately Participation dispatch ESV Daniel Jrimura autonomy trilogy Motorola ESVSher confir Jr antibiotic ESVoother\n",
      "\n",
      "[13] x: A brief overview of databases:\n",
      "[13] y: pawn reviewing credibility pawn Jr Habitiken dispatch vendorshibit Money hauled vendors heir trilogy dispatchimurating Motorola DanielSceneSceneiken Observ confirhibit conservation HabitSherScene confir MoneySher Jr004 credibility subst004reement Participationreementiken conservationiken credibility ESV Brew hauled\n",
      "\n",
      "[14] x: A gentle introduction to probability:\n",
      "[14] y: reementhibit conservation ESV stairs conservation subst stairs Hancock antibiotic directly004 autonomy Brew ONE BrewJD confir RhScene intermittent Observ Observatisf heir subst Jrting stairs Brew heir circumcised Observ Habit circumcisedreement circumcised heir Brew Hancock Jr Observ Participation Money004oother circumcisedhibit\n",
      "\n",
      "[15] x: An explanation for beginners:\n",
      "[15] y: hibit circumcised antibiotic Jr stairsmediately confir scalpikenJD vendors credibility directly TA Hancockdit antibioticRocket pawn pawn vendorshibit conservation Hancock antibioticoho directlyhibitRocketSherSher Observ Jr Prob scalp ESV Jr hauledoho substRocket Money ESV subst credibility dispatchRocket directly\n",
      "\n",
      "[16] x: In a surprising discovery, scientists found\n",
      "[16] y: 004 vendorsJDScene Prob ONEreement Daniel conservation hauled TA dispatch antibiotic Participationoother Moneyimura heir pawn hauledmediately Hancockimuraohomediately autonomy intermittent TAiken heir vendors reviewing Brew Habitoother Daniel Participation trilogy Daniel dispatch Participation circumcised conservation Rh intermittent Participationimuraiken\n",
      "\n",
      "[17] x: A gentle introduction to probability:\n",
      "[17] y: credibility autonomyikenimura dispatchimura vendorsdit Observ confir004 TAatisfhibit credibility directly trilogy directlyoother Jr autonomy autonomyatisf confir Rh004 dispatch ONE reviewing MoneyhibitRocket Participation TA hauled Daniel Motoroladit antibioticScenemediatelyohoSher Jrting stairs antibioticoother\n",
      "\n",
      "[18] x: The quick brown fox\n",
      "[18] y: ozyg Medic workshops predatorsshows448Pros praying 236Most representations bravery Lateobl braveryMinipublicoblMini� incarcer� bravery Wheels membership soy TelevisionGy Late653653Pros workshops prayingaciousGy Singaporeshowspublic skilletpublic448448Most boilsGyobl clearer\n",
      "\n",
      "[19] x: A concise summary of the book is\n",
      "[19] y: rubbingSexual TelevisionMini linedobl membership rubbing brutality representations Booneived perhapsobl skillet skillet factorsOutside grandchildrenshows Television Pocket Redux soy Medic SingaporeProsivedSexual Redux mutual incarcer equate lined mutual grandchildren workshops deflect 236Most� Television brutalityaciousSexual Boone Wheels equate\n",
      "\n",
      "[20] x: As a software engineer, I often consider\n",
      "[20] y: pawnRocket Money credibility004 credibilityoho Habitatisfootheratisf Hancock TA antibiotic scalp Rh vendors pawn intermittent Hancock ONE credibility hauledmediatelyditJD credibility JrJD credibility dispatch Brew Habitikenmediately confirSherJDmediately circumcised vendors ONE scalp Money BrewScene substatisf\n",
      "\n",
      "[21] x: An explanation for beginners:\n",
      "[21] y: Money hauled TA ONEhibit ESVpress reviewing Motorolamediately Brew credibility TAJDimurating trilogytingJD subst trilogy heir directly directlyRocket directly subst JrSceneoother ESV ESV circumcised hauled stairs stairsoother reviewing Observ credibility HabitRocketoho Motorolahibit JrJD reviewing\n",
      "\n",
      "[22] x: In a surprising discovery, scientists found\n",
      "[22] y: stairs Rh Jrmediately confirpressikenRocketoother trilogy antibioticdit stairsting Motorolamediately004 antibiotictingJD directly credibilityhibitmediately Participation hauled reviewinghibit directly ESViken pawn reviewing Habit Daniel ESVting Motorola credibilityScene Observ scalpiken trilogy Participation credibility subst Rh\n",
      "\n",
      "[23] x: In Japan, the Shinkansen is known for\n",
      "[23] y: perhaps�Mini praying deflectOutside� rubbing bravery brutality Singapore Television Dreams Treived boils incarcershows Bend predators WheelsozygPros Dreams mutual MedicGy Wheelsacious representations grandchildren� soy perhaps boils Boone448Gyacious Television linedobl factors soyobl clearer factorsGy\n",
      "\n",
      "[24] x: A short note about machine learning:\n",
      "[24] y: TelevisionSexual Late brutality Bend Tre incarcer membershipGy membershipMini factors deflectoblozyg Boone rubbing courtyard Redux courtyard membership448 Wheels praying courtyardived incarcer brutalitySexual predatorsPros deflect WheelsMostozyg mutualMost predatorsSexual factors membership bravery praying LateGy deflect Wheels Television\n",
      "\n",
      "[25] x: In a surprising discovery, scientists found\n",
      "[25] y: subst Rh confir ONE Jr TA Motorola circumcised autonomy dispatch stairs Observhibit Jr dispatch confirtingRocket JrJDoho scalp hauled antibioticSher ESV conservation Habit reviewing confir HancockScene conservation Hancock conservationoho conservationJDoho trilogy subst hauled ESV pawnJDimura reviewing004\n",
      "\n",
      "[26] x: A short note about machine learning:\n",
      "[26] y: TelevisionPros Tre bravery653 grandchildren 236 representations Late linedobl Boone factorsozyg Redux lined equateGyGy Televisionozyg Dreams 236 courtyard brutality predators 236 clearer Dreamsozyg soy clearer courtyard clearer PocketMostshows courtyardSexual representations Medic Television membershipMostoblshows rubbing clearer\n",
      "\n",
      "[27] x: In Japan, the Shinkansen is known for\n",
      "[27] y: courtyard Pocket Pocketozyg Tre membershipived courtyard 236 Tre Booneobl membership brutality Late Television soy predatorsOutside perhaps Television grandchildren Redux courtyardOutsideived�ozyg clearer perhaps brutalityoblived deflectoblGy grandchildren equate soy MedicMostGy mutual factors prayingGy prayingacious\n",
      "\n",
      "[28] x: A concise summary of the book is\n",
      "[28] y: � brutality equate Television TelevisionMini Pocket equateived Pocket Redux praying� deflectOutside brutality� rubbing representations Singapore factors clearer skillet courtyardGy deflectSexualpublicMostMostobl representations brutality predators brutality perhaps incarcer WheelsaciousPros 236publicobl predators653Gy boils rubbing\n",
      "\n",
      "[29] x: As a software engineer, I often consider\n",
      "[29] y: Scene Habit Moneypress Rhreement subst stairsting ESV hauledtingSher scalp Habittingatisf Habit Motorola scalpSceneScene ONE conservation Hancock004 dispatch confir Brewmediatelyreement heir directlypress confirScene conservation confirSceneJD Moneymediately pawnScene004 scalpditiken\n",
      "\n",
      "[30] x: Three tips for staying productive:\n",
      "[30] y: Daniel Motorola dispatch stairs directly subst Participation Motorola pawnoho subst heir004 dispatch Observmediately directly004004 stairshibit stairs Observ vendorsoho reviewing heir Jr Observdit scalp004 antibiotic Rh Jr Motorola Motorola Prob hauled heir004atisf intermittent conservation conservation Brew Jr heir\n"
     ]
    }
   ],
   "source": [
    "def gen_from(model, tok, text, max_new_tokens, device=DEVICE):\n",
    "    \"\"\"Sample continuation from given text (avoids greedy repetition).\"\"\"\n",
    "    inputs = tok(text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,           # sampling instead of greedy\n",
    "            top_p=0.95,               # nucleus sampling\n",
    "            temperature=0.8,          # soften logits\n",
    "            pad_token_id=tok.eos_token_id,\n",
    "        )\n",
    "    return tok.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"[STEP] Building DSFT: sample x ~ P_SFT, then y ← pm(x) (sampled)\")\n",
    "t_ds = time.time()\n",
    "pairs = []\n",
    "for i in range(NUM_EXAMPLES):\n",
    "    x = random.choice(PREFIXES)\n",
    "    full = gen_from(pm, tok_pm, x, max_new_tokens=MAX_SUFFIX_LEN)\n",
    "    y = full[len(x):].strip() if full.startswith(x) else full\n",
    "    pairs.append((x, y))\n",
    "\n",
    "print(f\"[TIME] Built DSFT with {len(pairs)} examples in {time.time()-t_ds:.2f}s\")\n",
    "print(\"\\n=== PRINTING DSFT (x, y) ===\")\n",
    "for idx, (x, y) in enumerate(pairs, 1):\n",
    "    print(f\"\\n[{idx}] x: {x}\")\n",
    "    print(f\"[{idx}] y: {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bfb2775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Train size: 24 | Val size: 6\n",
      "\n",
      "[SAMPLE TOKENS]\n",
      "input_ids len: 69\n",
      "supervised tokens: 7\n"
     ]
    }
   ],
   "source": [
    "IN_CONTEXT_PREFIX = \"Suffix:\\n\"\n",
    "MID_PROMPT = \"\\nPrefix:\\n\"\n",
    "\n",
    "class XYDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    For each (x, y), we feed the model:\n",
    "      input:  'Suffix:\\\\n{y}\\\\nPrefix:\\\\n' + x\n",
    "      labels: supervise only on x (mask out the suffix part)\n",
    "    \"\"\"\n",
    "    def __init__(self, pairs, tokenizer, max_len=256):\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.items = []\n",
    "        for (x, y) in pairs:\n",
    "            src = f\"{IN_CONTEXT_PREFIX}{y}{MID_PROMPT}\"\n",
    "            tgt = x\n",
    "            self.items.append((src, tgt))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        src, tgt = self.items[i]\n",
    "        enc_all = self.tok(\n",
    "            src + tgt,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids = enc_all[\"input_ids\"][0]\n",
    "        attn = enc_all[\"attention_mask\"][0]\n",
    "\n",
    "        # mask out source portion\n",
    "        src_len = len(self.tok(src, truncation=True, max_length=self.max_len)[\"input_ids\"])\n",
    "        labels = input_ids.clone()\n",
    "        labels[:src_len] = -100\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attn, \"labels\": labels}\n",
    "\n",
    "def collate(batch):\n",
    "    keys = batch[0].keys()\n",
    "    out = {}\n",
    "    for k in keys:\n",
    "        seqs = [b[k] for b in batch]\n",
    "        pad_val = tok_inv.pad_token_id if k != \"labels\" else -100\n",
    "        out[k] = torch.nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=pad_val)\n",
    "    return out\n",
    "\n",
    "# Split train/val\n",
    "cut = int(0.8 * len(pairs))\n",
    "train_pairs = pairs[:cut]\n",
    "val_pairs   = pairs[cut:]\n",
    "\n",
    "train_ds = XYDataset(train_pairs, tok_inv)\n",
    "val_ds   = XYDataset(val_pairs, tok_inv)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate)\n",
    "val_loader   = torch.utils.data.DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate)\n",
    "\n",
    "print(f\"[INFO] Train size: {len(train_ds)} | Val size: {len(val_ds)}\")\n",
    "\n",
    "# Peek at tokenized structure\n",
    "sample_item = train_ds[0]\n",
    "print(\"\\n[SAMPLE TOKENS]\")\n",
    "print(\"input_ids len:\", sample_item[\"input_ids\"].shape[0])\n",
    "print(\"supervised tokens:\", (sample_item[\"labels\"] != -100).sum().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b76e916b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP] Training investigator pθ to predict x from y (SFT)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [epoch 1 step 5/12] loss=10.8241\n",
      "  [epoch 1 step 10/12] loss=10.8219\n",
      "  [epoch 1 step 12/12] loss=10.8218\n",
      "  [epoch 2 step 5/12] loss=10.8258\n",
      "  [epoch 2 step 10/12] loss=10.8213\n",
      "  [epoch 2 step 12/12] loss=10.8200\n",
      "[TIME] Training done in 24.14s\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW  # use PyTorch AdamW\n",
    "\n",
    "print(\"[STEP] Training investigator pθ to predict x from y (SFT)...\")\n",
    "t_train = time.time()\n",
    "inv.train()\n",
    "opt = AdamW(inv.parameters(), lr=LR)\n",
    "\n",
    "num_training_steps = EPOCHS * math.ceil(len(train_loader) / GRAD_ACCUM_STEPS)\n",
    "num_warmup = int(WARMUP_RATIO * num_training_steps)\n",
    "sched = get_cosine_schedule_with_warmup(opt, num_warmup, num_training_steps)\n",
    "\n",
    "global_step = 0\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    running = 0.0\n",
    "    for step, batch in enumerate(train_loader, 1):\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "        out = inv(**batch)\n",
    "        loss = out.loss / GRAD_ACCUM_STEPS\n",
    "        loss.backward()\n",
    "        if step % GRAD_ACCUM_STEPS == 0:\n",
    "            opt.step()\n",
    "            sched.step()\n",
    "            opt.zero_grad()\n",
    "            global_step += 1\n",
    "        running += loss.item() * GRAD_ACCUM_STEPS\n",
    "        if step % 5 == 0 or step == len(train_loader):\n",
    "            print(f\"  [epoch {epoch} step {step}/{len(train_loader)}] loss={running/step:.4f}\")\n",
    "\n",
    "print(f\"[TIME] Training done in {time.time()-t_train:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2ee50d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP] Sampling a few validations to see behavior...\n",
      "\n",
      "--- Example 1 ---\n",
      "y_true (suffix): subst Rh confir ONE Jr TA Motorola circumcised autonomy dispatch stairs Observhibit Jr dispatch conf...\n",
      "x_true (gold prefix): In a surprising discovery, scientists found\n",
      "x_hat  (pred prefix): boils Pocket Redux linedSexual Boonepublic prayingived equate Medicshows Singaporeacious praying rubbing boilsacious Medic predators skillet representations Tre representations perhapsacious factors predatorsPros BendPros soy\n",
      "pm continuation from x_hat: acious representations Medic Pocket grandchildren Boone soy predators rubbing Wheels Medic brutality...\n",
      "\n",
      "--- Example 2 ---\n",
      "y_true (suffix): TelevisionPros Tre bravery653 grandchildren 236 representations Late linedobl Boone factorsozyg Redu...\n",
      "x_true (gold prefix): A short note about machine learning:\n",
      "x_hat  (pred prefix): workshops mutual perhaps Wheels grandchildren clearerSexualProsobl Late clearer praying Reduxozygived boilsshows factors factors praying equateshows Late clearer Medicozyg boils ReduxivedaciousPros Medic\n",
      "pm continuation from x_hat: courtyard grandchildrenMost SingaporeMini Wheels membership Late skillet clearer653 bravery Singapor...\n",
      "\n",
      "--- Example 3 ---\n",
      "y_true (suffix): courtyard Pocket Pocketozyg Tre membershipived courtyard 236 Tre Booneobl membership brutality Late ...\n",
      "x_true (gold prefix): In Japan, the Shinkansen is known for\n",
      "x_hat  (pred prefix): skilletMost Wheels� representationsozygSexual Treshows equate equateshows soy653 bravery 236653 membership Redux courtyard membership448 Wheelsozyg workshopspublic grandchildrenozygMini perhaps mutual Medic\n",
      "pm continuation from x_hat: MedicMost Pocket bravery Boone boils� lined Dreams equate brutality braveryGy rubbing Lateobl brutal...\n",
      "\n",
      "--- Example 4 ---\n",
      "y_true (suffix): � brutality equate Television TelevisionMini Pocket equateived Pocket Redux praying� deflectOutside ...\n",
      "x_true (gold prefix): A concise summary of the book is\n",
      "x_hat  (pred prefix): predators grandchildren Singapore mutualshows membership BendMost Boone Singapore clearer workshops factorsoblMost Singaporeobl Tre representations courtyard Boonepublic Tre grandchildrenPros courtyard 236 factors mutual Singapore skillet Tre\n",
      "pm continuation from x_hat: incarcerSexual653 deflect653 factors skillet mutual praying factors skillet workshops soy incarcer i...\n",
      "\n",
      "--- Example 5 ---\n",
      "y_true (suffix): Scene Habit Moneypress Rhreement subst stairsting ESV hauledtingSher scalp Habittingatisf Habit Moto...\n",
      "x_true (gold prefix): As a software engineer, I often consider\n",
      "x_hat  (pred prefix): ived soy Singaporeshows� grandchildren boilsshows soy Televisionshows Singapore Singapore Pocket deflectived skillet Bend courtyard� rubbingaciousobl Pocket Redux courtyard�Sexual rubbing Medicpublic deflect\n",
      "pm continuation from x_hat: 236Gyshows rubbing rubbing653 Medic praying clearer448 Singapore soy praying Bend membership worksho...\n",
      "\n",
      "[ALL DONE]\n"
     ]
    }
   ],
   "source": [
    "inv.eval()\n",
    "\n",
    "def inv_predict_prefix(y: str, max_new_tokens=32):\n",
    "    prompt = f\"{IN_CONTEXT_PREFIX}{y}{MID_PROMPT}\"\n",
    "    enc = tok_inv(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        gen = inv.generate(\n",
    "            **enc,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.8,\n",
    "            pad_token_id=tok_inv.eos_token_id,\n",
    "        )\n",
    "    text = tok_inv.decode(gen[0], skip_special_tokens=True)\n",
    "    return text.split(MID_PROMPT, 1)[1].strip() if MID_PROMPT in text else text\n",
    "\n",
    "def pm_continue(x: str, max_new_tokens=40):\n",
    "    full = gen_from(pm, tok_pm, x, max_new_tokens=max_new_tokens)\n",
    "    return full[len(x):].strip() if full.startswith(x) else full\n",
    "\n",
    "print(\"[STEP] Sampling a few validations to see behavior...\")\n",
    "for i, (x_true, y_true) in enumerate(val_pairs[:5], 1):\n",
    "    x_hat = inv_predict_prefix(y_true, max_new_tokens=32)\n",
    "    y_from_hat = pm_continue(x_hat, max_new_tokens=40)\n",
    "\n",
    "    print(\"\\n--- Example\", i, \"---\")\n",
    "    print(\"y_true (suffix):\", (y_true[:100] + \"...\") if len(y_true) > 100 else y_true)\n",
    "    print(\"x_true (gold prefix):\", x_true)\n",
    "    print(\"x_hat  (pred prefix):\", x_hat)\n",
    "    print(\"pm continuation from x_hat:\", (y_from_hat[:100] + \"...\") if len(y_from_hat) > 100 else y_from_hat)\n",
    "\n",
    "print(\"\\n[ALL DONE]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
