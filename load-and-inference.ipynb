{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0845c530",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seanzhou/Documents/llm research/sft-dpo-fw/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Recreate SFT step (toy) for investigator pθ(x|y)\n",
    "import os, time, math, random\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, get_cosine_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# ---- Config ----\n",
    "SEED = 42\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "PM_ID  = \"sshleifer/tiny-gpt2\"     # target LM pm (forward x->y)\n",
    "INV_ID = \"sshleifer/tiny-gpt2\"     # investigator base pθ (we'll fine-tune)\n",
    "\n",
    "# Tiny prefix distribution P_SFT\n",
    "PREFIXES = [\n",
    "    \"A short note about machine learning:\",\n",
    "    \"Three tips for staying productive:\",\n",
    "    \"A gentle introduction to probability:\",\n",
    "    \"In a surprising discovery, scientists found\",\n",
    "    \"As a software engineer, I often consider\",\n",
    "    \"In Japan, the Shinkansen is known for\",\n",
    "    \"A concise summary of the book is\",\n",
    "    \"The quick brown fox\",\n",
    "    \"An explanation for beginners:\",\n",
    "    \"A brief overview of databases:\"\n",
    "]\n",
    "\n",
    "NUM_EXAMPLES   = 30     # size of DSFT\n",
    "MAX_PREFIX_EXT = 8      # optional: extend x slightly for variety\n",
    "MAX_SUFFIX_LEN = 48     # greedy suffix length\n",
    "BATCH_SIZE     = 2\n",
    "EPOCHS         = 2\n",
    "LR             = 5e-5\n",
    "WARMUP_RATIO   = 0.1\n",
    "GRAD_ACCUM_STEPS = 1\n",
    "\n",
    "# Repro\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"[INFO] Device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04b4d155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP] Loading tokenizer (pm)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP] Loading target model pm (x->y)... (first run may download)\n",
      "[STEP] Loading tokenizer (investigator pθ)...\n",
      "[STEP] Loading investigator base model pθ (will fine-tune)...\n",
      "[TIME] Models loaded in 7.43s\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "print(\"[STEP] Loading tokenizer (pm)...\")\n",
    "tok_pm = AutoTokenizer.from_pretrained(PM_ID)\n",
    "if tok_pm.pad_token is None:\n",
    "    tok_pm.pad_token = tok_pm.eos_token\n",
    "\n",
    "print(\"[STEP] Loading target model pm (x->y)... (first run may download)\")\n",
    "pm = AutoModelForCausalLM.from_pretrained(\n",
    "    PM_ID,\n",
    "    use_safetensors=True,          # avoids .bin load\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float32\n",
    ").to(DEVICE)\n",
    "pm.eval()\n",
    "\n",
    "print(\"[STEP] Loading tokenizer (investigator pθ)...\")\n",
    "tok_inv = AutoTokenizer.from_pretrained(INV_ID)\n",
    "if tok_inv.pad_token is None:\n",
    "    tok_inv.pad_token = tok_inv.eos_token\n",
    "\n",
    "print(\"[STEP] Loading investigator base model pθ (will fine-tune)...\")\n",
    "inv = AutoModelForCausalLM.from_pretrained(\n",
    "    INV_ID,\n",
    "    use_safetensors=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float32\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"[TIME] Models loaded in {time.time()-t0:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d74fec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP] Building DSFT: sample x ~ P_SFT, then y ← pm(x) (sampled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seanzhou/Documents/llm research/sft-dpo-fw/.venv/lib/python3.10/site-packages/transformers/pytorch_utils.py:339: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_elements = torch.tensor(test_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIME] Built DSFT with 30 examples in 88.12s\n",
      "\n",
      "=== PRINTING DSFT (x, y) ===\n",
      "\n",
      "[1] x: Three tips for staying productive:\n",
      "[1] y: reement Brew intermittentatisf TA Prob Prob MotorolaScene circumcisedohoatisfimura ObservimuraJD intermittent stairsikenoother Rhdit Brew Habitatisf autonomy antibiotic heirootheroother Daniel Rh trilogy reviewingting Brewpress ESV trilogy MoneyJD antibiotic pawn Prob conservation Hancockdit vendors\n",
      "\n",
      "[2] x: A short note about machine learning:\n",
      "[2] y: Sexual membershipPros clearer653aciousMostozyg factors predators equate brutality Pocketozyg Pocket Late incarcerSexual praying workshopsMiniivedshows grandchildrenozyg448Prosshows courtyard workshops Boone soyivedpublic predatorsSexual Tre Tre Wheels 236 perhaps lined 236�Mini rubbingPros equate\n",
      "\n",
      "[3] x: As a software engineer, I often consider\n",
      "[3] y: press reviewing Danieltingpress004 hauled pawn hauled TA ONE Hancock Jr ESV antibiotic stairsting hauled vendors credibility credibility MoneyJD Observ Prob antibioticdit dispatch MoneyRocket Daniel Brew directly directly Habit vendorshibitmediatelytingoho dispatchditimura DanielRocketimurapressJD\n",
      "\n",
      "[4] x: In a surprising discovery, scientists found\n",
      "[4] y: stairsSher Observ ProbJD circumcisedmediately BrewRocket Habit scalp Daniel subst Moneyatisfmediately Daniel credibility Motorola ONE heir confirRocketScene intermittent trilogyoother Jr Daniel heir credibility reviewing conservation Observikenting Habit Brew dispatchmediately Moneymediatelydit HabitJDdit Hancock Daniel\n",
      "\n",
      "[5] x: In a surprising discovery, scientists found\n",
      "[5] y: vendorsdit antibioticScene reviewing Observ scalphibithibitpress vendors Hancock TARocket Jriken reviewingtingditSher ProbreementtingmediatelyoothermediatelyRocket Hancock ObservScene stairs trilogyatisfScene TAreementpress Rh004 conservation antibiotic Hancockhibit stairs pawn stairs autonomy reviewing\n",
      "\n",
      "[6] x: A gentle introduction to probability:\n",
      "[6] y: scalp antibiotic confir Observ Prob scalp Participation vendors directly ESV vendorsmediately substimura circumcised credibility Prob intermittentpressoho Jr heirpress Participation pawn heir RhScene stairs confir trilogy stairs autonomyatisf Jrmediately004 ONERocketiken Observ Habit Jr intermittent Participation Motorola TARocket\n",
      "\n",
      "[7] x: Three tips for staying productive:\n",
      "[7] y: Observ hauled reviewing hauledhibit intermittent credibility circumcised reviewing Jr Rh confir Brew ProbRocket circumcised Prob Brew reviewing ONE ONE ONE vendors trilogy Rh Habitimura subst vendors trilogy autonomy JrRocket dispatchreement antibiotic Prob Danieliken autonomy Money Rh heir Jr stairs confir circumcised directly\n",
      "\n",
      "[8] x: An explanation for beginners:\n",
      "[8] y: oho BrewRocket Daniel Rh hauled Participation circumcised conservation circumcisediken pawn ESV circumcised credibilityScenemediatelyimura Daniel ONEScene antibiotic conservation heir hauled Danielootherpress Motorola004 autonomy reviewing antibiotic ONE autonomyRocket reviewing Motorola MoneyScene Jr circumcised ONE confir stairsimuratingdit\n",
      "\n",
      "[9] x: Three tips for staying productive:\n",
      "[9] y: reviewing Participationimura Daniel Hancockatisf confir autonomyohopresshibitoho TA Habit trilogy Participationatisf scalpohopress ONE subst stairsmediately directly credibility TASceneimuratingRocket Participationimura ProbSherdit stairs dispatchoho004 TAiken conservation intermittentiken004 dispatch hauled\n",
      "\n",
      "[10] x: A brief overview of databases:\n",
      "[10] y: imura Money antibioticRocketmediately stairs stairs subst Hancock scalp Rh Prob ONE intermittentikenScene Jriken conservation Jr ESV dispatch Daniel heir ONE HabitJDhibit ONE antibiotic autonomy004 circumcisedatisf pawn substhibit Jr intermittent directly ESV Observhibit Rh trilogy subst ONE dispatch\n",
      "\n",
      "[11] x: A concise summary of the book is\n",
      "[11] y: Tre Redux Tre BendOutsideOutside Singapore deflectSexual Redux Medicived Tre Medic grandchildren 236 linedMini Medic deflect bravery courtyard boils incarcerMini soy SingaporeMini Pocket Singapore workshops SingaporeoblozygMini WheelsOutside membership Dreams brutalityozyg incarcer653 brutality�shows rubbing Tre\n",
      "\n",
      "[12] x: A short note about machine learning:\n",
      "[12] y: � Boone bravery deflectaciousshowsived grandchildren653 deflect Medic factorspublic Dreams Television Singapore skillet factors predatorspublic Medic linedMini membershipSexualshows Pocket Television448 skillet perhaps praying skillet mutual653 courtyardOutside Television equate soy Wheels boils 236ozygGypublic grandchildren Tre\n",
      "\n",
      "[13] x: A short note about machine learning:\n",
      "[13] y: TelevisionSexual BooneMini boils factors courtyardacious448 workshopsshowsMost membership653 equate predators653Most skillet Late bravery Dreams Dreams 236 equate soy clearerOutside�ozyg braveryozyg Late factors clearer workshopsProsPros lined membershipived deflect predators Pocket Boone courtyard Lateived\n",
      "\n",
      "[14] x: Three tips for staying productive:\n",
      "[14] y: reementhibit conservation ESV stairs conservation subst stairs Hancock antibiotic directly004 autonomy Brew ONE BrewJD confir RhScene intermittent Observ Observatisf heir subst Jrting stairs Brew heir circumcised Observ Habit circumcisedreement circumcised heir Brew Hancock Jr Observ Participation Money004oother circumcisedhibit\n",
      "\n",
      "[15] x: In a surprising discovery, scientists found\n",
      "[15] y: hibit circumcised antibiotic Jr stairsmediately confir scalpikenJD vendors credibility directly TA Hancockdit antibioticRocket pawn pawn vendorshibit conservation Hancock antibioticoho directlyhibitRocketSherSher Observ Jr Prob scalp ESV Jr hauledoho substRocket Money ESV subst credibility dispatchRocket directly\n",
      "\n",
      "[16] x: In a surprising discovery, scientists found\n",
      "[16] y: 004 vendorsJDScene Prob ONEreement Daniel conservation hauled TA dispatch antibiotic Participationoother Moneyimura heir pawn hauledmediately Hancockimuraohomediately autonomy intermittent TAiken heir vendors reviewing Brew Habitoother Daniel Participation trilogy Daniel dispatch Participation circumcised conservation Rh intermittent Participationimuraiken\n",
      "\n",
      "[17] x: An explanation for beginners:\n",
      "[17] y: credibility autonomyikenimura dispatchimura vendorsdit Observ confir004 TAatisfhibit credibility directly trilogy directlyoother Jr autonomy autonomyatisf confir Rh004 dispatch ONE reviewing MoneyhibitRocket Participation TA hauled Daniel Motoroladit antibioticScenemediatelyohoSher Jrting stairs antibioticoother\n",
      "\n",
      "[18] x: A brief overview of databases:\n",
      "[18] y: ESVSherSher Daniel directly credibility trilogy Participation dispatch Hancock Motorola Jr Hancock trilogy stairs Brew004presshibit Motorola vendors Jroho004 confir stairs ESVimura confir Jr credibility dispatchiken vendorspress trilogy vendors stairs confir Jrohoreement Danieloother circumcised autonomy trilogy directly\n",
      "\n",
      "[19] x: A short note about machine learning:\n",
      "[19] y: rubbingSexual TelevisionMini linedobl membership rubbing brutality representations Booneived perhapsobl skillet skillet factorsOutside grandchildrenshows Television Pocket Redux soy Medic SingaporeProsivedSexual Redux mutual incarcer equate lined mutual grandchildren workshops deflect 236Most� Television brutalityaciousSexual Boone Wheels equate\n",
      "\n",
      "[20] x: An explanation for beginners:\n",
      "[20] y: pawnRocket Money credibility004 credibilityoho Habitatisfootheratisf Hancock TA antibiotic scalp Rh vendors pawn intermittent Hancock ONE credibility hauledmediatelyditJD credibility JrJD credibility dispatch Brew Habitikenmediately confirSherJDmediately circumcised vendors ONE scalp Money BrewScene substatisf\n",
      "\n",
      "[21] x: In a surprising discovery, scientists found\n",
      "[21] y: Money hauled TA ONEhibit ESVpress reviewing Motorolamediately Brew credibility TAJDimurating trilogytingJD subst trilogy heir directly directlyRocket directly subst JrSceneoother ESV ESV circumcised hauled stairs stairsoother reviewing Observ credibility HabitRocketoho Motorolahibit JrJD reviewing\n",
      "\n",
      "[22] x: An explanation for beginners:\n",
      "[22] y: stairs Rh Jrmediately confirpressikenRocketoother trilogy antibioticdit stairsting Motorolamediately004 antibiotictingJD directly credibilityhibitmediately Participation hauled reviewinghibit directly ESViken pawn reviewing Habit Daniel ESVting Motorola credibilityScene Observ scalpiken trilogy Participation credibility subst Rh\n",
      "\n",
      "[23] x: A concise summary of the book is\n",
      "[23] y: perhaps�Mini praying deflectOutside� rubbing bravery brutality Singapore Television Dreams Treived boils incarcershows Bend predators WheelsozygPros Dreams mutual MedicGy Wheelsacious representations grandchildren� soy perhaps boils Boone448Gyacious Television linedobl factors soyobl clearer factorsGy\n",
      "\n",
      "[24] x: In a surprising discovery, scientists found\n",
      "[24] y: intermittent004 ESV subst pawn dispatchJDreementtingimura scalp hauled Habit Prob vendors circumcised heir Motorola directly intermittentimura vendorsmediatelyoother directly ParticipationSher ESV Daniel dispatchSher ESVoho Motorolamediatelyiken vendorsreementatisf antibiotic antibiotic reviewingoho hauled conservationoother stairs Prob\n",
      "\n",
      "[25] x: The quick brown fox\n",
      "[25] y: � predators equate Singapore equate Redux Wheels bravery grandchildren bravery Booneobl boils factors representationsozyg workshops representations soy deflectshowsMost� workshops mutualacious� Pocket653 mutual� equate Wheels Late mutualived rubbing perhaps incarcer grandchildren workshops clearer Wheels factors rubbing soy brutality equate\n",
      "\n",
      "[26] x: A brief overview of databases:\n",
      "[26] y: imura substSher004iken Participation trilogy ParticipationSher ONE Observ ESV scalp004imura autonomydit Observ Motorola confiratisf antibiotic dispatch pawnimura vendors pawn scalp Prob autonomyreement Habit ONEoho circumcisedatisf intermittent Motorolaoho trilogy Hancock ESV directlyRocketting Jr Prob subst\n",
      "\n",
      "[27] x: As a software engineer, I often consider\n",
      "[27] y: heir Jr stairs reviewing Prob intermittent HabitditRocket Jratisf TAScene stairs Participationiken dispatchmediatelyreement Rh Motorola Jr autonomy credibility subst conservation pawnikenditreement circumcisedhibit Money confir Participationting MotorolamediatelySheratisf conservation Danielimura subst Brewoother circumcisedimura\n",
      "\n",
      "[28] x: A short note about machine learning:\n",
      "[28] y: � brutality equate Television TelevisionMini Pocket equateived Pocket Redux praying� deflectOutside brutality� rubbing representations Singapore factors clearer skillet courtyardGy deflectSexualpublicMostMostobl representations brutality predators brutality perhaps incarcer WheelsaciousPros 236publicobl predators653Gy boils rubbing\n",
      "\n",
      "[29] x: A gentle introduction to probability:\n",
      "[29] y: Scene Habit Moneypress Rhreement subst stairsting ESV hauledtingSher scalp Habittingatisf Habit Motorola scalpSceneScene ONE conservation Hancock004 dispatch confir Brewmediatelyreement heir directlypress confirScene conservation confirSceneJD Moneymediately pawnScene004 scalpditiken\n",
      "\n",
      "[30] x: A concise summary of the book is\n",
      "[30] y: grandchildren653 Bend Bend brutality membershipOutsideMini praying braveryPros Television perhapsshows Treshows factors mutualobl factorsobl448 skillet rubbing courtyard brutalitypublicMiniMiniobl bravery skillet Television clearer skillet Boone skilletSexualSexualMost courtyard bravery incarcer rubbing courtyard Medic Bend bravery\n"
     ]
    }
   ],
   "source": [
    "def gen_from(model, tok, text, max_new_tokens, device=DEVICE):\n",
    "    \"\"\"Sample continuation from given text (avoids greedy repetition).\"\"\"\n",
    "    inputs = tok(text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,           # sampling instead of greedy\n",
    "            top_p=0.95,               # nucleus sampling\n",
    "            temperature=0.8,          # soften logits\n",
    "            pad_token_id=tok.eos_token_id,\n",
    "        )\n",
    "    return tok.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"[STEP] Building DSFT: sample x ~ P_SFT, then y ← pm(x) (sampled)\")\n",
    "t_ds = time.time()\n",
    "pairs = []\n",
    "for i in range(NUM_EXAMPLES):\n",
    "    x = random.choice(PREFIXES)\n",
    "    full = gen_from(pm, tok_pm, x, max_new_tokens=MAX_SUFFIX_LEN)\n",
    "    y = full[len(x):].strip() if full.startswith(x) else full\n",
    "    pairs.append((x, y))\n",
    "\n",
    "print(f\"[TIME] Built DSFT with {len(pairs)} examples in {time.time()-t_ds:.2f}s\")\n",
    "print(\"\\n=== PRINTING DSFT (x, y) ===\")\n",
    "for idx, (x, y) in enumerate(pairs, 1):\n",
    "    print(f\"\\n[{idx}] x: {x}\")\n",
    "    print(f\"[{idx}] y: {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bfb2775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Train size: 24 | Val size: 6\n",
      "\n",
      "[SAMPLE TOKENS]\n",
      "input_ids len: 68\n",
      "supervised tokens: 6\n"
     ]
    }
   ],
   "source": [
    "IN_CONTEXT_PREFIX = \"Suffix:\\n\"\n",
    "MID_PROMPT = \"\\nPrefix:\\n\"\n",
    "\n",
    "class XYDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    For each (x, y), we feed the model:\n",
    "      input:  'Suffix:\\\\n{y}\\\\nPrefix:\\\\n' + x\n",
    "      labels: supervise only on x (mask out the suffix part)\n",
    "    \"\"\"\n",
    "    def __init__(self, pairs, tokenizer, max_len=256):\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.items = []\n",
    "        for (x, y) in pairs:\n",
    "            src = f\"{IN_CONTEXT_PREFIX}{y}{MID_PROMPT}\"\n",
    "            tgt = x\n",
    "            self.items.append((src, tgt))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        src, tgt = self.items[i]\n",
    "        enc_all = self.tok(\n",
    "            src + tgt,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids = enc_all[\"input_ids\"][0]\n",
    "        attn = enc_all[\"attention_mask\"][0]\n",
    "\n",
    "        # mask out source portion\n",
    "        src_len = len(self.tok(src, truncation=True, max_length=self.max_len)[\"input_ids\"])\n",
    "        labels = input_ids.clone()\n",
    "        labels[:src_len] = -100\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attn, \"labels\": labels}\n",
    "\n",
    "def collate(batch):\n",
    "    keys = batch[0].keys()\n",
    "    out = {}\n",
    "    for k in keys:\n",
    "        seqs = [b[k] for b in batch]\n",
    "        pad_val = tok_inv.pad_token_id if k != \"labels\" else -100\n",
    "        out[k] = torch.nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=pad_val)\n",
    "    return out\n",
    "\n",
    "# Split train/val\n",
    "cut = int(0.8 * len(pairs))\n",
    "train_pairs = pairs[:cut]\n",
    "val_pairs   = pairs[cut:]\n",
    "\n",
    "train_ds = XYDataset(train_pairs, tok_inv)\n",
    "val_ds   = XYDataset(val_pairs, tok_inv)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate)\n",
    "val_loader   = torch.utils.data.DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate)\n",
    "\n",
    "print(f\"[INFO] Train size: {len(train_ds)} | Val size: {len(val_ds)}\")\n",
    "\n",
    "# Peek at tokenized structure\n",
    "sample_item = train_ds[0]\n",
    "print(\"\\n[SAMPLE TOKENS]\")\n",
    "print(\"input_ids len:\", sample_item[\"input_ids\"].shape[0])\n",
    "print(\"supervised tokens:\", (sample_item[\"labels\"] != -100).sum().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b76e916b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP] Training investigator pθ to predict x from y (SFT)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [epoch 1 step 5/12] loss=10.8279\n",
      "  [epoch 1 step 10/12] loss=10.8254\n",
      "  [epoch 1 step 12/12] loss=10.8246\n",
      "  [epoch 2 step 5/12] loss=10.8298\n",
      "  [epoch 2 step 10/12] loss=10.8268\n",
      "  [epoch 2 step 12/12] loss=10.8252\n",
      "[TIME] Training done in 8.78s\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW  # use PyTorch AdamW\n",
    "\n",
    "print(\"[STEP] Training investigator pθ to predict x from y (SFT)...\")\n",
    "t_train = time.time()\n",
    "inv.train()\n",
    "opt = AdamW(inv.parameters(), lr=LR)\n",
    "\n",
    "num_training_steps = EPOCHS * math.ceil(len(train_loader) / GRAD_ACCUM_STEPS)\n",
    "num_warmup = int(WARMUP_RATIO * num_training_steps)\n",
    "sched = get_cosine_schedule_with_warmup(opt, num_warmup, num_training_steps)\n",
    "\n",
    "global_step = 0\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    running = 0.0\n",
    "    for step, batch in enumerate(train_loader, 1):\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "        out = inv(**batch)\n",
    "        loss = out.loss / GRAD_ACCUM_STEPS\n",
    "        loss.backward()\n",
    "        if step % GRAD_ACCUM_STEPS == 0:\n",
    "            opt.step()\n",
    "            sched.step()\n",
    "            opt.zero_grad()\n",
    "            global_step += 1\n",
    "        running += loss.item() * GRAD_ACCUM_STEPS\n",
    "        if step % 5 == 0 or step == len(train_loader):\n",
    "            print(f\"  [epoch {epoch} step {step}/{len(train_loader)}] loss={running/step:.4f}\")\n",
    "\n",
    "print(f\"[TIME] Training done in {time.time()-t_train:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2ee50d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP] Sampling a few validations to see behavior...\n",
      "\n",
      "--- Example 1 ---\n",
      "y_true (suffix): � predators equate Singapore equate Redux Wheels bravery grandchildren bravery Booneobl boils factor...\n",
      "x_true (gold prefix): The quick brown fox\n",
      "x_hat  (pred prefix): shows equate rubbing Pocket praying Bend 236 236 praying factors Late boils Television653 equate lined boils boils equate Pocket Pocket Pocket skillet Singapore Pocket representations Dreams bravery Tre Bend skillet Boone\n",
      "pm continuation from x_hat: ived workshops skillet rubbing Television mutual predatorsOutside clearer perhapsPros deflectobl cle...\n",
      "\n",
      "--- Example 2 ---\n",
      "y_true (suffix): imura substSher004iken Participation trilogy ParticipationSher ONE Observ ESV scalp004imura autonomy...\n",
      "x_true (gold prefix): A brief overview of databases:\n",
      "x_hat  (pred prefix): factors Treozygozyg Pocket LateOutside lined DreamsaciousSexualived membership representations grandchildren soyMini boils braveryozyg perhaps soy soy predators representations Television equate predatorsPros grandchildren�ozyg\n",
      "pm continuation from x_hat: lined boilsobl Bend equate predators Late clearer Pocket incarcerozygshowsMost rubbingpublic equate6...\n",
      "\n",
      "--- Example 3 ---\n",
      "y_true (suffix): heir Jr stairs reviewing Prob intermittent HabitditRocket Jratisf TAScene stairs Participationiken d...\n",
      "x_true (gold prefix): As a software engineer, I often consider\n",
      "x_hat  (pred prefix): JDJD directly Jr004 TA reviewing dispatch credibility Danielhibit dispatch heir intermittentting antibioticmediately Hancock confir subst autonomy credibility autonomy antibiotic Brewmediatelyoho TA Daniel heirRocketdit\n",
      "pm continuation from x_hat: imura intermittent pawn credibility Brew pawn004oho HabitJD conservationreementhibit Hancock Motorol...\n",
      "\n",
      "--- Example 4 ---\n",
      "y_true (suffix): � brutality equate Television TelevisionMini Pocket equateived Pocket Redux praying� deflectOutside ...\n",
      "x_true (gold prefix): A short note about machine learning:\n",
      "x_hat  (pred prefix): ozyg653publicOutside mutual Television Boone MedicPros perhapsshows workshops lined mutual Late Wheels representations soy Wheels deflect653ived BendozygMost� Tre factors Dreams predators soy Wheels\n",
      "pm continuation from x_hat: workshops�Sexual bravery�MiniozygivedSexual Bendaciousozyg 236 praying incarcer653 skillet Bend Boon...\n",
      "\n",
      "--- Example 5 ---\n",
      "y_true (suffix): Scene Habit Moneypress Rhreement subst stairsting ESV hauledtingSher scalp Habittingatisf Habit Moto...\n",
      "x_true (gold prefix): A gentle introduction to probability:\n",
      "x_hat  (pred prefix): prayingacious448653acious� perhapsMinishowsSexual� grandchildren membership equateshows�MostMiniSexual soy 236 Medicpublic Late Boone rubbing Boone Wheels653 Television clearerMost\n",
      "pm continuation from x_hat: rubbing SingaporeSexualived Redux praying praying clearer Boone mutualoblPros workshopsMiniacious653...\n",
      "\n",
      "[ALL DONE]\n"
     ]
    }
   ],
   "source": [
    "inv.eval()\n",
    "\n",
    "def inv_predict_prefix(y: str, max_new_tokens=32):\n",
    "    prompt = f\"{IN_CONTEXT_PREFIX}{y}{MID_PROMPT}\"\n",
    "    enc = tok_inv(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        gen = inv.generate(\n",
    "            **enc,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.8,\n",
    "            pad_token_id=tok_inv.eos_token_id,\n",
    "        )\n",
    "    text = tok_inv.decode(gen[0], skip_special_tokens=True)\n",
    "    return text.split(MID_PROMPT, 1)[1].strip() if MID_PROMPT in text else text\n",
    "\n",
    "def pm_continue(x: str, max_new_tokens=40):\n",
    "    full = gen_from(pm, tok_pm, x, max_new_tokens=max_new_tokens)\n",
    "    return full[len(x):].strip() if full.startswith(x) else full\n",
    "\n",
    "print(\"[STEP] Sampling a few validations to see behavior...\")\n",
    "for i, (x_true, y_true) in enumerate(val_pairs[:5], 1):\n",
    "    x_hat = inv_predict_prefix(y_true, max_new_tokens=32)\n",
    "    y_from_hat = pm_continue(x_hat, max_new_tokens=40)\n",
    "\n",
    "    print(\"\\n--- Example\", i, \"---\")\n",
    "    print(\"y_true (suffix):\", (y_true[:100] + \"...\") if len(y_true) > 100 else y_true)\n",
    "    print(\"x_true (gold prefix):\", x_true)\n",
    "    print(\"x_hat  (pred prefix):\", x_hat)\n",
    "    print(\"pm continuation from x_hat:\", (y_from_hat[:100] + \"...\") if len(y_from_hat) > 100 else y_from_hat)\n",
    "\n",
    "print(\"\\n[ALL DONE]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d247addb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP] Constructing DPO preference data for 5 suffixes...\n",
      "[INFO] Built 5 DPO pairs (max 5).\n",
      "[EXAMPLE DPO ITEM]\n",
      "prompt (truncated): Suffix:\n",
      "credibility autonomyikenimura dispatchimura vendorsdit Observ confir004 TAatisfhibit credibility directly trilog...\n",
      "chosen: TelevisionOutside Wheels perhaps soypublicpublicSexualacious grandchildren perhaps BendSexualOutside mutual skilletived� predators clearer representations soy Bendived Redux ReduxMost Late factors skillet grandchildren Pocket bravery equate Late ReduxMostoblOutside braveryMini Singapore Tre mutual equateSexual Medic lined\n",
      "rejected: skillet praying 236 boils representationsobl equate membershipshows WheelsPros grandchildrenProsacious Medic Redux predatorsacious boilsMost membership perhapsPros448 ReduxPros equateMiniPros Tre653 lined workshopsshowsacious Medic653Mini equate factors boilsived deflect boilsozygived Late equate\n"
     ]
    }
   ],
   "source": [
    "import random, torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def inv_sample_prefix(y: str, max_new_tokens=48):\n",
    "    \"\"\"\n",
    "    Sample a candidate prefix x ~ pθ(.|y) from the current investigator `inv`.\n",
    "    Uses the same input template as SFT: 'Suffix:\\\\n{y}\\\\nPrefix:\\\\n'.\n",
    "    \"\"\"\n",
    "    prompt = f\"{IN_CONTEXT_PREFIX}{y}{MID_PROMPT}\"\n",
    "    enc = tok_inv(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    inv.eval()\n",
    "    with torch.no_grad():\n",
    "        out = inv.generate(\n",
    "            **enc,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,       # sample to get diverse candidates\n",
    "            top_p=0.9,\n",
    "            temperature=0.8,\n",
    "            pad_token_id=tok_inv.eos_token_id,\n",
    "        )\n",
    "    text = tok_inv.decode(out[0], skip_special_tokens=True)\n",
    "    # Extract the continuation after the 'Prefix:' marker\n",
    "    return text.split(MID_PROMPT, 1)[1].strip() if MID_PROMPT in text else text.strip()\n",
    "\n",
    "def logprob_y_given_x(pm, tok, x: str, y: str):\n",
    "    \"\"\"\n",
    "    Compute log p_m(y | x) under the target LM `pm` via teacher forcing.\n",
    "    Returns: (sum_logprob, avg_logprob_per_token, num_y_tokens).\n",
    "    \"\"\"\n",
    "    if not y:\n",
    "        return float(\"-inf\"), float(\"-inf\"), 0\n",
    "\n",
    "    # Tokenize full sequence and the split point (where y begins)\n",
    "    ids_full = tok(x + y, return_tensors=\"pt\").input_ids[0].to(DEVICE)  # [T]\n",
    "    ids_x    = tok(x,     return_tensors=\"pt\").input_ids[0].to(DEVICE)  # [Tx]\n",
    "    start = ids_x.shape[0]  # index in ids_full where y starts\n",
    "    if start >= ids_full.shape[0]:\n",
    "        return float(\"-inf\"), float(\"-inf\"), 0\n",
    "\n",
    "    pm.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = pm(ids_full.unsqueeze(0)).logits[0]   # [T, V]\n",
    "        logp   = F.log_softmax(logits, dim=-1)         # [T, V]\n",
    "\n",
    "    # Next-token logprobs for each position: log P(token[t] | tokens[:t])\n",
    "    # Align by shifting targets by one\n",
    "    next_token_logp = logp[:-1, :].gather(1, ids_full[1:].unsqueeze(-1)).squeeze(-1)  # [T-1]\n",
    "\n",
    "    # y occupies ids_full[start:], so predictions for y are at positions [start-1 : end-1)\n",
    "    y_logp = next_token_logp[start-1:]\n",
    "    sum_lp = float(y_logp.sum().item())\n",
    "    n_tok  = int(y_logp.shape[0])\n",
    "    avg_lp = (sum_lp / n_tok) if n_tok > 0 else float(\"-inf\")\n",
    "    return sum_lp, avg_lp, n_tok\n",
    "\n",
    "# ----- Config for DPO data construction (smaller for demo) -----\n",
    "K_CANDIDATES = 2        # 2 prefixes per suffix\n",
    "NUM_DPO_SUFFIXES = 5    # only use 5 suffixes total\n",
    "MAX_INV_GEN = 48\n",
    "SEED = 123\n",
    "random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "# take only a small pool of suffixes\n",
    "suffix_pool = [y for (x, y) in pairs if isinstance(y, str) and len(y) > 0]\n",
    "random.shuffle(suffix_pool)\n",
    "suffix_pool = suffix_pool[:NUM_DPO_SUFFIXES]\n",
    "\n",
    "dpo_triples = []\n",
    "print(f\"[STEP] Constructing DPO preference data for {len(suffix_pool)} suffixes...\")\n",
    "\n",
    "for y in suffix_pool:\n",
    "    cands = []\n",
    "    for _ in range(K_CANDIDATES):\n",
    "        x = inv_sample_prefix(y)\n",
    "        if not x or len(x.strip()) < 3:\n",
    "            continue\n",
    "        sum_lp, avg_lp, n_tok = logprob_y_given_x(pm, tok_pm, x, y)\n",
    "        cands.append((x, sum_lp, avg_lp, n_tok))\n",
    "    if len(cands) < 2:\n",
    "        continue\n",
    "\n",
    "    # pick one winner and one loser\n",
    "    cands.sort(key=lambda t: t[1], reverse=True)\n",
    "    winner = cands[0][0]\n",
    "    loser  = cands[-1][0]\n",
    "\n",
    "    dpo_triples.append({\n",
    "        \"prompt\": f\"{IN_CONTEXT_PREFIX}{y}{MID_PROMPT}\",\n",
    "        \"chosen\": winner,\n",
    "        \"rejected\": loser,\n",
    "    })\n",
    "\n",
    "print(f\"[INFO] Built {len(dpo_triples)} DPO pairs (max {NUM_DPO_SUFFIXES}).\")\n",
    "if dpo_triples:\n",
    "    ex = dpo_triples[0]\n",
    "    print(\"[EXAMPLE DPO ITEM]\")\n",
    "    print(\"prompt (truncated):\", (ex['prompt'][:120] + \"...\") if len(ex['prompt'])>120 else ex['prompt'])\n",
    "    print(\"chosen:\", ex[\"chosen\"])\n",
    "    print(\"rejected:\", ex[\"rejected\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
