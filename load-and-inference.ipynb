{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0845c530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Recreate SFT step (toy) for investigator pθ(x|y)\n",
    "import os, time, math, random\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, get_cosine_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# ---- Config ----\n",
    "SEED = 42\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "PM_ID  = \"sshleifer/tiny-gpt2\"     # target LM pm (forward x->y)\n",
    "INV_ID = \"sshleifer/tiny-gpt2\"     # investigator base pθ (we'll fine-tune)\n",
    "\n",
    "# Tiny prefix distribution P_SFT\n",
    "PREFIXES = [\n",
    "    \"A short note about machine learning:\",\n",
    "    \"Three tips for staying productive:\",\n",
    "    \"A gentle introduction to probability:\",\n",
    "    \"In a surprising discovery, scientists found\",\n",
    "    \"As a software engineer, I often consider\",\n",
    "    \"In Japan, the Shinkansen is known for\",\n",
    "    \"A concise summary of the book is\",\n",
    "    \"The quick brown fox\",\n",
    "    \"An explanation for beginners:\",\n",
    "    \"A brief overview of databases:\"\n",
    "]\n",
    "\n",
    "NUM_EXAMPLES   = 30     # size of DSFT\n",
    "MAX_PREFIX_EXT = 8      # optional: extend x slightly for variety\n",
    "MAX_SUFFIX_LEN = 48     # greedy suffix length\n",
    "BATCH_SIZE     = 2\n",
    "EPOCHS         = 2\n",
    "LR             = 5e-5\n",
    "WARMUP_RATIO   = 0.1\n",
    "GRAD_ACCUM_STEPS = 1\n",
    "\n",
    "# Repro\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"[INFO] Device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04b4d155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP] Loading tokenizer (pm)...\n",
      "[STEP] Loading target model pm (x->y)... (first run may download)\n",
      "[STEP] Loading tokenizer (investigator pθ)...\n",
      "[STEP] Loading investigator base model pθ (will fine-tune)...\n",
      "[TIME] Models loaded in 15.99s\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "print(\"[STEP] Loading tokenizer (pm)...\")\n",
    "tok_pm = AutoTokenizer.from_pretrained(PM_ID)\n",
    "if tok_pm.pad_token is None:\n",
    "    tok_pm.pad_token = tok_pm.eos_token\n",
    "\n",
    "print(\"[STEP] Loading target model pm (x->y)... (first run may download)\")\n",
    "pm = AutoModelForCausalLM.from_pretrained(\n",
    "    PM_ID,\n",
    "    use_safetensors=True,          # avoids .bin load\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float32\n",
    ").to(DEVICE)\n",
    "pm.eval()\n",
    "\n",
    "print(\"[STEP] Loading tokenizer (investigator pθ)...\")\n",
    "tok_inv = AutoTokenizer.from_pretrained(INV_ID)\n",
    "if tok_inv.pad_token is None:\n",
    "    tok_inv.pad_token = tok_inv.eos_token\n",
    "\n",
    "print(\"[STEP] Loading investigator base model pθ (will fine-tune)...\")\n",
    "inv = AutoModelForCausalLM.from_pretrained(\n",
    "    INV_ID,\n",
    "    use_safetensors=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float32\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"[TIME] Models loaded in {time.time()-t0:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d74fec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP] Building DSFT: sample x ~ P_SFT, then y ← pm(x) (sampled)\n",
      "[TIME] Built DSFT with 30 examples in 70.12s\n",
      "\n",
      "=== PRINTING DSFT (x, y) ===\n",
      "\n",
      "[1] x: In a surprising discovery, scientists found\n",
      "[1] y: reement Brew intermittentatisf TA Prob Prob MotorolaScene circumcisedohoatisfimura ObservimuraJD intermittent stairsikenoother Rhdit Brew Habitatisf autonomy antibiotic heirootheroother Daniel Rh trilogy reviewingting Brewpress ESV trilogy MoneyJD antibiotic pawn Prob conservation Hancockdit vendors\n",
      "\n",
      "[2] x: Three tips for staying productive:\n",
      "[2] y: Sher vendors circumcised vendorsScene antibiotic Observ Observ dispatch directly Motorola Jr stairsoothertingJD Jr heirmediatelyikendit Rhatisfting credibilityootherreement substimuraoother heir Money autonomyootherhibitditRocket hauled Observ confirJD Daniel dispatch Brewatisfoother Hancockiken\n",
      "\n",
      "[3] x: A concise summary of the book is\n",
      "[3] y: Boone grandchildren boils praying skillet linedSexualOutsideobl equate brutalityGy courtyard Pocketpublic deflectMini clearer� membership incarcer skilletMini Singapore incarcer Television grandchildrenshows praying clearershowsMost incarcerobl representations448 Television Television Booneaciousobl Television membership predators perhapsSexualobl Boone\n",
      "\n",
      "[4] x: As a software engineer, I often consider\n",
      "[4] y: stairsSher Observ ProbJD circumcisedmediately BrewRocket Habit scalp Daniel subst Moneyatisfmediately Daniel credibility Motorola ONE heir confirRocketScene intermittent trilogyoother Jr Daniel heir credibility reviewing conservation Observikenting Habit Brew dispatchmediately Moneymediatelydit HabitJDdit Hancock Daniel\n",
      "\n",
      "[5] x: The quick brown fox\n",
      "[5] y: Wheels courtyard Boone courtyardSexual workshops factors 236 equate boilsived Televisionobl membership TreSexual equate factors lined grandchildren Pocket bravery Pocket workshopsPros clearerOutside deflect 236Sexual brutalityozyg Late factors Television Singapore653Most skillet factors653 rubbing� deflectSexual courtyard Singapore deflect\n",
      "\n",
      "[6] x: In Japan, the Shinkansen is known for\n",
      "[6] y: Outside Treshows skilletozyg TreGy boils LateMiniMini Singapore mutualPros incarcer clearer BooneozygProsshows clearer incarcer soyMost Redux Wheels653 workshopsSexual448ozyg predatorsobl representationsozygshows Bend� brutalityPros courtyardived predatorsobl mutual courtyard 236Pros\n",
      "\n",
      "[7] x: A gentle introduction to probability:\n",
      "[7] y: Observ hauled reviewing hauledhibit intermittent credibility circumcised reviewing Jr Rh confir Brew ProbRocket circumcised Prob Brew reviewing ONE ONE ONE vendors trilogy Rh Habitimura subst vendors trilogy autonomy JrRocket dispatchreement antibiotic Prob Danieliken autonomy Money Rh heir Jr stairs confir circumcised directly\n",
      "\n",
      "[8] x: In Japan, the Shinkansen is known for\n",
      "[8] y: 448ived courtyard workshops factorsGy653 courtyardozyg Tre predatorsGy Medic Boone equateMini SingaporeivedSexual factors predators linedPros Tre deflectshows Medic Bend incarcer 236 perhaps Tre Late praying lined factors skillet perhaps Medic Medicobl factors representations 236653 bravery Singapore membership\n",
      "\n",
      "[9] x: In Japan, the Shinkansen is known for\n",
      "[9] y: Wheelsoblshows Medic Singapore Singaporeoblozyg boils brutality braveryMini Bend�public prayingived rubbing factors Singapore predators braveryozygGy linedobl Bend praying praying membershippublic lined clearer clearer grandchildren� BoonePros incarcer Tre Boone653 equate skilletOutsideMini factorsGy\n",
      "\n",
      "[10] x: In a surprising discovery, scientists found\n",
      "[10] y: imura Money antibioticRocketmediately stairs stairs subst Hancock scalp Rh Prob ONE intermittentikenScene Jriken conservation Jr ESV dispatch Daniel heir ONE HabitJDhibit ONE antibiotic autonomy004 circumcisedatisf pawn substhibit Jr intermittent directly ESV Observhibit Rh trilogy subst ONE dispatch\n",
      "\n",
      "[11] x: As a software engineer, I often consider\n",
      "[11] y: Rocket vendorsting Probimura trilogy ProbRocket credibilitypress vendors Motorola directlymediately Hancock Jr Jr antibiotic scalpmediately stairs antibiotic MoneyJDpress reviewingoho JrSher vendors Brew004 Hancock autonomy vendorsJD Observ Brewoother ESV Moneydit DanielSceneoother directly dispatch intermittent\n",
      "\n",
      "[12] x: Three tips for staying productive:\n",
      "[12] y: oother Prob ESV MotorolaRocket credibility Habit antibiotic subst Habit pawn reviewing Danielditoother dispatch Prob autonomypress ESV antibiotic ONEoho vendorsoother confirpresshibit vendorsmediatelyreementmediately Participation dispatch ESV Daniel Jrimura autonomy trilogy Motorola ESVSher confir Jr antibiotic ESVoother\n",
      "\n",
      "[13] x: A brief overview of databases:\n",
      "[13] y: pawn reviewing credibility pawn Jr Habitiken dispatch vendorshibit Money hauled vendors heir trilogy dispatchimurating Motorola DanielSceneSceneiken Observ confirhibit conservation HabitSherScene confir MoneySher Jr004 credibility subst004reement Participationreementiken conservationiken credibility ESV Brew hauled\n",
      "\n",
      "[14] x: A gentle introduction to probability:\n",
      "[14] y: reementhibit conservation ESV stairs conservation subst stairs Hancock antibiotic directly004 autonomy Brew ONE BrewJD confir RhScene intermittent Observ Observatisf heir subst Jrting stairs Brew heir circumcised Observ Habit circumcisedreement circumcised heir Brew Hancock Jr Observ Participation Money004oother circumcisedhibit\n",
      "\n",
      "[15] x: An explanation for beginners:\n",
      "[15] y: hibit circumcised antibiotic Jr stairsmediately confir scalpikenJD vendors credibility directly TA Hancockdit antibioticRocket pawn pawn vendorshibit conservation Hancock antibioticoho directlyhibitRocketSherSher Observ Jr Prob scalp ESV Jr hauledoho substRocket Money ESV subst credibility dispatchRocket directly\n",
      "\n",
      "[16] x: In a surprising discovery, scientists found\n",
      "[16] y: 004 vendorsJDScene Prob ONEreement Daniel conservation hauled TA dispatch antibiotic Participationoother Moneyimura heir pawn hauledmediately Hancockimuraohomediately autonomy intermittent TAiken heir vendors reviewing Brew Habitoother Daniel Participation trilogy Daniel dispatch Participation circumcised conservation Rh intermittent Participationimuraiken\n",
      "\n",
      "[17] x: A gentle introduction to probability:\n",
      "[17] y: credibility autonomyikenimura dispatchimura vendorsdit Observ confir004 TAatisfhibit credibility directly trilogy directlyoother Jr autonomy autonomyatisf confir Rh004 dispatch ONE reviewing MoneyhibitRocket Participation TA hauled Daniel Motoroladit antibioticScenemediatelyohoSher Jrting stairs antibioticoother\n",
      "\n",
      "[18] x: The quick brown fox\n",
      "[18] y: ozyg Medic workshops predatorsshows448Pros praying 236Most representations bravery Lateobl braveryMinipublicoblMini� incarcer� bravery Wheels membership soy TelevisionGy Late653653Pros workshops prayingaciousGy Singaporeshowspublic skilletpublic448448Most boilsGyobl clearer\n",
      "\n",
      "[19] x: A concise summary of the book is\n",
      "[19] y: rubbingSexual TelevisionMini linedobl membership rubbing brutality representations Booneived perhapsobl skillet skillet factorsOutside grandchildrenshows Television Pocket Redux soy Medic SingaporeProsivedSexual Redux mutual incarcer equate lined mutual grandchildren workshops deflect 236Most� Television brutalityaciousSexual Boone Wheels equate\n",
      "\n",
      "[20] x: As a software engineer, I often consider\n",
      "[20] y: pawnRocket Money credibility004 credibilityoho Habitatisfootheratisf Hancock TA antibiotic scalp Rh vendors pawn intermittent Hancock ONE credibility hauledmediatelyditJD credibility JrJD credibility dispatch Brew Habitikenmediately confirSherJDmediately circumcised vendors ONE scalp Money BrewScene substatisf\n",
      "\n",
      "[21] x: An explanation for beginners:\n",
      "[21] y: Money hauled TA ONEhibit ESVpress reviewing Motorolamediately Brew credibility TAJDimurating trilogytingJD subst trilogy heir directly directlyRocket directly subst JrSceneoother ESV ESV circumcised hauled stairs stairsoother reviewing Observ credibility HabitRocketoho Motorolahibit JrJD reviewing\n",
      "\n",
      "[22] x: In a surprising discovery, scientists found\n",
      "[22] y: stairs Rh Jrmediately confirpressikenRocketoother trilogy antibioticdit stairsting Motorolamediately004 antibiotictingJD directly credibilityhibitmediately Participation hauled reviewinghibit directly ESViken pawn reviewing Habit Daniel ESVting Motorola credibilityScene Observ scalpiken trilogy Participation credibility subst Rh\n",
      "\n",
      "[23] x: In Japan, the Shinkansen is known for\n",
      "[23] y: perhaps�Mini praying deflectOutside� rubbing bravery brutality Singapore Television Dreams Treived boils incarcershows Bend predators WheelsozygPros Dreams mutual MedicGy Wheelsacious representations grandchildren� soy perhaps boils Boone448Gyacious Television linedobl factors soyobl clearer factorsGy\n",
      "\n",
      "[24] x: A short note about machine learning:\n",
      "[24] y: TelevisionSexual Late brutality Bend Tre incarcer membershipGy membershipMini factors deflectoblozyg Boone rubbing courtyard Redux courtyard membership448 Wheels praying courtyardived incarcer brutalitySexual predatorsPros deflect WheelsMostozyg mutualMost predatorsSexual factors membership bravery praying LateGy deflect Wheels Television\n",
      "\n",
      "[25] x: In a surprising discovery, scientists found\n",
      "[25] y: subst Rh confir ONE Jr TA Motorola circumcised autonomy dispatch stairs Observhibit Jr dispatch confirtingRocket JrJDoho scalp hauled antibioticSher ESV conservation Habit reviewing confir HancockScene conservation Hancock conservationoho conservationJDoho trilogy subst hauled ESV pawnJDimura reviewing004\n",
      "\n",
      "[26] x: A short note about machine learning:\n",
      "[26] y: TelevisionPros Tre bravery653 grandchildren 236 representations Late linedobl Boone factorsozyg Redux lined equateGyGy Televisionozyg Dreams 236 courtyard brutality predators 236 clearer Dreamsozyg soy clearer courtyard clearer PocketMostshows courtyardSexual representations Medic Television membershipMostoblshows rubbing clearer\n",
      "\n",
      "[27] x: In Japan, the Shinkansen is known for\n",
      "[27] y: courtyard Pocket Pocketozyg Tre membershipived courtyard 236 Tre Booneobl membership brutality Late Television soy predatorsOutside perhaps Television grandchildren Redux courtyardOutsideived�ozyg clearer perhaps brutalityoblived deflectoblGy grandchildren equate soy MedicMostGy mutual factors prayingGy prayingacious\n",
      "\n",
      "[28] x: A concise summary of the book is\n",
      "[28] y: � brutality equate Television TelevisionMini Pocket equateived Pocket Redux praying� deflectOutside brutality� rubbing representations Singapore factors clearer skillet courtyardGy deflectSexualpublicMostMostobl representations brutality predators brutality perhaps incarcer WheelsaciousPros 236publicobl predators653Gy boils rubbing\n",
      "\n",
      "[29] x: As a software engineer, I often consider\n",
      "[29] y: Scene Habit Moneypress Rhreement subst stairsting ESV hauledtingSher scalp Habittingatisf Habit Motorola scalpSceneScene ONE conservation Hancock004 dispatch confir Brewmediatelyreement heir directlypress confirScene conservation confirSceneJD Moneymediately pawnScene004 scalpditiken\n",
      "\n",
      "[30] x: Three tips for staying productive:\n",
      "[30] y: Daniel Motorola dispatch stairs directly subst Participation Motorola pawnoho subst heir004 dispatch Observmediately directly004004 stairshibit stairs Observ vendorsoho reviewing heir Jr Observdit scalp004 antibiotic Rh Jr Motorola Motorola Prob hauled heir004atisf intermittent conservation conservation Brew Jr heir\n"
     ]
    }
   ],
   "source": [
    "def gen_from(model, tok, text, max_new_tokens, device=DEVICE):\n",
    "    \"\"\"Sample continuation from given text (avoids greedy repetition).\"\"\"\n",
    "    inputs = tok(text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,           # sampling instead of greedy\n",
    "            top_p=0.95,               # nucleus sampling\n",
    "            temperature=0.8,          # soften logits\n",
    "            pad_token_id=tok.eos_token_id,\n",
    "        )\n",
    "    return tok.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"[STEP] Building DSFT: sample x ~ P_SFT, then y ← pm(x) (sampled)\")\n",
    "t_ds = time.time()\n",
    "pairs = []\n",
    "for i in range(NUM_EXAMPLES):\n",
    "    x = random.choice(PREFIXES)\n",
    "    full = gen_from(pm, tok_pm, x, max_new_tokens=MAX_SUFFIX_LEN)\n",
    "    y = full[len(x):].strip() if full.startswith(x) else full\n",
    "    pairs.append((x, y))\n",
    "\n",
    "print(f\"[TIME] Built DSFT with {len(pairs)} examples in {time.time()-t_ds:.2f}s\")\n",
    "print(\"\\n=== PRINTING DSFT (x, y) ===\")\n",
    "for idx, (x, y) in enumerate(pairs, 1):\n",
    "    print(f\"\\n[{idx}] x: {x}\")\n",
    "    print(f\"[{idx}] y: {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bfb2775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Train size: 24 | Val size: 6\n",
      "\n",
      "[SAMPLE TOKENS]\n",
      "input_ids len: 69\n",
      "supervised tokens: 7\n"
     ]
    }
   ],
   "source": [
    "IN_CONTEXT_PREFIX = \"Suffix:\\n\"\n",
    "MID_PROMPT = \"\\nPrefix:\\n\"\n",
    "\n",
    "class XYDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    For each (x, y), we feed the model:\n",
    "      input:  'Suffix:\\\\n{y}\\\\nPrefix:\\\\n' + x\n",
    "      labels: supervise only on x (mask out the suffix part)\n",
    "    \"\"\"\n",
    "    def __init__(self, pairs, tokenizer, max_len=256):\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.items = []\n",
    "        for (x, y) in pairs:\n",
    "            src = f\"{IN_CONTEXT_PREFIX}{y}{MID_PROMPT}\"\n",
    "            tgt = x\n",
    "            self.items.append((src, tgt))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        src, tgt = self.items[i]\n",
    "        enc_all = self.tok(\n",
    "            src + tgt,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids = enc_all[\"input_ids\"][0]\n",
    "        attn = enc_all[\"attention_mask\"][0]\n",
    "\n",
    "        # mask out source portion\n",
    "        src_len = len(self.tok(src, truncation=True, max_length=self.max_len)[\"input_ids\"])\n",
    "        labels = input_ids.clone()\n",
    "        labels[:src_len] = -100\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attn, \"labels\": labels}\n",
    "\n",
    "def collate(batch):\n",
    "    keys = batch[0].keys()\n",
    "    out = {}\n",
    "    for k in keys:\n",
    "        seqs = [b[k] for b in batch]\n",
    "        pad_val = tok_inv.pad_token_id if k != \"labels\" else -100\n",
    "        out[k] = torch.nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=pad_val)\n",
    "    return out\n",
    "\n",
    "# Split train/val\n",
    "cut = int(0.8 * len(pairs))\n",
    "train_pairs = pairs[:cut]\n",
    "val_pairs   = pairs[cut:]\n",
    "\n",
    "train_ds = XYDataset(train_pairs, tok_inv)\n",
    "val_ds   = XYDataset(val_pairs, tok_inv)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate)\n",
    "val_loader   = torch.utils.data.DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate)\n",
    "\n",
    "print(f\"[INFO] Train size: {len(train_ds)} | Val size: {len(val_ds)}\")\n",
    "\n",
    "# Peek at tokenized structure\n",
    "sample_item = train_ds[0]\n",
    "print(\"\\n[SAMPLE TOKENS]\")\n",
    "print(\"input_ids len:\", sample_item[\"input_ids\"].shape[0])\n",
    "print(\"supervised tokens:\", (sample_item[\"labels\"] != -100).sum().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39d37073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seanzhou/Documents/llm research/sft-dpo-fw/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Loading model (this may take a bit on first run, downloads weights)...\n",
      "Step 3: Defining prompts...\n",
      "Step 4: Running generations...\n",
      "\n",
      "--- Generating for prompt 1: \"Once upon a time\" ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seanzhou/Documents/llm research/sft-dpo-fw/.venv/lib/python3.10/site-packages/transformers/pytorch_utils.py:339: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_elements = torch.tensor(test_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time incarcer equateacious Singapore predators perhapsPros workshopsMostPros 236ived Boone LateOutside Medic lined boils Dreams factors grandchildren incarcer Medicaciousshows Tre skillet Wheels factors clearerSexualobl workshops equate Bend WheelsMost Dreamsobl workshops\n",
      "\n",
      "--- Generating for prompt 2: \"In a shocking discovery\" ---\n",
      "In a shocking discovery praying perhapspublic membership 236� soyshows Boone perhaps workshops boils predatorsPros bravery mutual skilletacious653 workshops equate representations Late Pocket brutalityMini Pocket deflect653Outside Redux grandchildrenMiniozyg workshops representations prayingGy predators Pocket\n",
      "\n",
      "--- Generating for prompt 3: \"The quick brown fox\" ---\n",
      "The quick brown fox Wheels Dreams SingaporeGy Bend Late TelevisionPros Redux bravery courtyard�448 Singaporeobl clearer WheelsaciousSexualived 236 Pocketobl brutalityMini membership factors Bend clearerobl mutual predators�acious 236 courtyardozyg soy courtyard grandchildren\n",
      "\n",
      "--- Generating for prompt 4: \"As a software engineer, I\" ---\n",
      "As a software engineer, I Jr Money Hancockpressoother stairstingikenpress Money Hancock hauled vendors Danielreement antibiotic directly confirpress vendors hauled circumcisedatisf Observ credibility intermittent ONEohoJD Observpress credibility antibiotic directly directlyatisf Habit Brew RhRocket\n",
      "\n",
      "--- Generating for prompt 5: \"In Japan, the Shinkansen\" ---\n",
      "In Japan, the Shinkansendit hauled substSher Participation MoneyRocket Habit substtingimura Participation dispatch Danieloother Habithibitditoho Participation autonomy Hancockmediately heirimura Habit Rh Prob confirSher reviewingoho directly Probatisfoho pawn Danielpressreement\n",
      "\n",
      "All generations complete.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_ID = \"sshleifer/tiny-gpt2\"  # very small test model\n",
    "\n",
    "print(\"Step 1: Loading tokenizer...\")\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "print(\"Step 2: Loading model (this may take a bit on first run, downloads weights)...\")\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    use_safetensors=True,   # ensure safe format\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float32,\n",
    ").to(device)\n",
    "\n",
    "print(\"Step 3: Defining prompts...\")\n",
    "prompts = [\n",
    "    \"Once upon a time\",\n",
    "    \"In a shocking discovery\",\n",
    "    \"The quick brown fox\",\n",
    "    \"As a software engineer, I\",\n",
    "    \"In Japan, the Shinkansen\",\n",
    "]\n",
    "\n",
    "print(\"Step 4: Running generations...\")\n",
    "for i, p in enumerate(prompts, 1):\n",
    "    print(f\"\\n--- Generating for prompt {i}: \\\"{p}\\\" ---\")\n",
    "    torch.manual_seed(100 + i)\n",
    "    x = tok(p, return_tensors=\"pt\").to(device)\n",
    "    y = model.generate(\n",
    "        **x,\n",
    "        max_new_tokens=40,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        temperature=0.9,\n",
    "        pad_token_id=tok.eos_token_id,\n",
    "    )\n",
    "    print(tok.decode(y[0], skip_special_tokens=True))\n",
    "\n",
    "print(\"\\nAll generations complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
